% Part: history
% Chapter: background
% Section: history

\documentclass[../../include/open-logic-section]{subfiles}

\begin{document}

\olfileid{inc}{bac}{his}

\olsection{Historical Background}

We have finally reached the ``incompleteness'' part of this course. In
this section, I will briefly discuss historical developments that will
help put the incompleteness theorems in context. In particular, I will
give a very sketchy and inadequate overview of the history of
mathematical logic; and then say a few words about the history of the
foundations of mathematics.

\begin{history}
  The phrase ``mathematical logic'' is ambiguous. One can interpret the
word ``mathematical'' as describing the subject matter, as in, ``the
logic of mathematics,'' denoting the principles of mathematical
reasoning; or as describing the methods, as in ``the mathematics of
logic,'' denoting a mathematical study of the principles of reasoning.
The account that follows involves mathematical logic in both senses,
often at the same time.
\end{history}

The study of logic began, essentially, with Aristotle, who lived
approximately 384--322 B.C. His \emph{Categories}, \emph{Prior
  analytics}, and \emph{Posterior analytics} include systematic
studies of the principles of scientific reasoning, including a
thorough and systematic study of the syllogism.

Aristotle's logic dominated scholastic philosophy through the middle
ages; indeed, as late as eighteenth century Kant maintained that
Aristotle's logic was perfect and in no need of revision. But the
theory of the syllogism is far too limited to model anything but
the most superficial aspects of mathematical reasoning. A century
earlier, Leibniz, a contemporary of Newton's, imagined a complete
``calculus'' for logical reasoning, and made some rudimentary steps
towards designing such a calculus, essentially describing a version of
propositional logic.

The nineteenth century was a watershed for logic. In 1854 George Boole
wrote \emph{The Laws of Thought}, with a thorough algebraic study of
propositional logic that is not far from modern presentations. In 1879
Gottlob Frege published his \emph{Begriffsschrift} (Concept writing)
which extends propositional logic with quantifiers and relations, and
thus includes first-order logic. In fact, Frege's logical systems
included higher-order logic as well, and more --- enough more to be
(as Russell showed in 1902) inconsistent. But setting aside the
inconsistent axiom, Frege more or less invented modern logic
singlehandedly, a startling achievement. Quantificational logic was
also developed independently by algebraically-minded thinkers after
Boole, including Peirce and Schr\"oder.

Let us now turn to developments in the foundations of mathematics. Of
course, since logic plays an important role in mathematics, there is a
good deal of interaction with the developments I just described. For
example, Frege developed his logic with the explicit purpose of
showing that all of mathematics could be based solely on his logical
framework; in particular, he wished to show that mathematics consists
of a priori \emph{analytic} truths instead of, as Kant had maintained,
a priori \emph{synthetic} ones.

Many take the birth of mathematics proper to have occurred with the
Greeks. Euclid's \emph{Elements}, written around 300 B.C., is already
a mature representative of Greek mathematics, with its emphasis on
rigor and precision. The definitions and proofs in Euclid's
\emph{Elements} survive more or less in tact in high school geometry
textbooks today (to the extent that geometry is still taught in high
schools). This model of mathematical reasoning has been held to be a
paradigm for rigorous argumentation not only in mathematics but in
branches of philosophy as well. (Spinoza even presented moral and
religious arguments in the Euclidean style, which is strange to see!)

Calculus was invented by Newton and Leibniz in the seventeenth
century. (A fierce priority dispute raged for centuries, but most
scholars today hold that the two developments were for the most part
independent.)  Calculus involves reasoning about, for example,
infinite sums of infinitely small quantities; these features fueled
criticism by Bishop Berkeley, who argued that belief in God was no
less rational than the mathematics of his time. The methods of
calculus were widely used in the eighteenth century, for example by
Leonhard Euler, who used calculations involving infinite sums with
dramatic results.

In the nineteenth century, mathematicians tried to address Berkeley's
criticisms by putting calculus on a firmer foundation. Efforts by
Cauchy, Weierstrass, Bolzano, and others led to our contemporary
definitions of limits, continuity, differentiation, and integration in
terms of ``epsilons and deltas,'' in other words, devoid of any
reference to infinitesimals. Later in the century, mathematicians
tried to push further, and explain all aspects of calculus, including
the real numbers themselves, in terms of the natural numbers.
(Kronecker: ``God created the whole numbers, all else is the work of
man.'') In 1872, Dedekind wrote ``Continuity and the irrational
numbers,'' where he showed how to ``construct'' the real numbers as
sets of rational numbers (which, as you know, can be viewed as pairs
of natural numbers); in 1888 he wrote ``Was sind und was sollen die
Zahlen'' (roughly, ``What are the natural numbers, and what should
they be?'') which aimed to explain the natural numbers in purely
``logical'' terms. In 1887 Kronecker wrote ``\"Uber den Zahlbegriff''
(``On the concept of number'') where he spoke of representing all
mathematical object in terms of the integers; in 1889 Giuseppe Peano
gave formal, symbolic axioms for the natural numbers.

The end of the nineteenth century also brought a new boldness in dealing
with the infinite. Before then, infinitary objects and structures
(like the set of natural numbers) were treated gingerly; ``infinitely
many'' was understood as ``as many as you want,'' and ``approaches in
the limit'' was understood as ``gets as close as you want.'' But Georg
Cantor showed that it was impossible to take the infinite at face
value. Work by Cantor, Dedekind, and others help to introduce the
general set-theoretic understanding of mathematics that we discussed
earlier in this course.

Which brings us to twentieth century developments in logic and foundations.
In 1902 Russell discovered the paradox in Frege's logical system. In
1904 Zermelo proved Cantor's well-ordering principle, using the
so-called ``axiom of choice''; the legitimacy of this axiom prompted a
good deal of debate. Between 1910 and 1913 the three volumes of
Russell and Whitehead's \emph{Principia Mathematica} appeared,
extending the Fregean program of establishing mathematics on logical
grounds. Unfortunately, Russell and Whitehead were forced to adopt two
principles that seemed hard to justify as purely logical: an axiom of
infinity and an axiom of ``reducibility.'' In the 1900's Poincar\'e
criticized the use of ``impredicative definitions'' in mathematics,
and in the 1910's Brouwer began proposing to refound all of
mathematics in an ``intuitionistic'' basis, which avoided the use of
the law of the excluded middle ($p \lor \lnot p$).

Strange days indeed\! The program of reducing all of mathematics to
logic is now referred to as ``logicism,'' and is commonly viewed as
having failed, due to the difficulties mentioned above. The program of
developing mathematics in terms of intuitionistic mental constructions
is called ``intuitionism,'' and is viewed as posing overly severe
restrictions on everyday mathematics. Around the turn of the century,
David Hilbert, one of the most influential mathematicians of all time,
was a strong supporter of the new, abstract methods introduced by
Cantor and Dedekind: ``no one will drive us from the paradise that
Cantor has created for us.'' At the same time, he was sensitive to
foundational criticisms of these new methods (oddly enough, now called
``classical''). He proposed a way of having one's cake and eating it
too:
\begin{itemize}
\item Represent classical methods with formal axioms and rules.
\item Use safe, ``finitary'' methods to prove that these formal
  deductive systems are consistent.
\end{itemize}
In 1931, G\"odel proved the two ``incompleteness theorems,'' which
showed that this program could not succeed. In this chapter, we will
discuss the incompleteness theorems in detail.

\end{document}
